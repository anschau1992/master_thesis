\documentclass{task_description}

\begin{document}

\thispagestyle{firstpage}
\vspace*{23mm}%
\hfill\parbox[t]{65mm}{

Andreas Schaufelb\"uhl\\
Binzm\"uhlestrasse 14\\
8050 Z\"urich\\[5mm]
Matrikel-Nr. 12-918-843\\
andreas.schaufelbuehl@uzh.com\\[15mm]
\today \\
}
\vspace*{5mm}
\subsection*{Master's Thesis Specification}
%

\section*{Titel: ???}


%
\subsection*{Introduction}
%
% Context
Natural machine translation [NMT] play a paramount role in the daily life of human beings. The usability of computational translated phrases is strictly bound to correctly and naturally translation close to these of professional human translator. Consequently, researcher put enormous effort in improving the state-of-the-art translation technologies, with the goal of producing natural accurate sentences. As it has the potential to support manual translation by increasing efficiency and reduce the amount of work drastic being done by humans in this area. \\
In recent years, the research of NMT have presented many different, novel models using Deep learning technologies. Using state of the art machine learning technologies, machine translation are able to produce more precise and natural translation and outperform existing stale models. Hokamp and Liu\cite{Hokamp} introduces the model of Grid Beam Search, allowing lexically constrained decoding of text, which improves the translation quality in interactive scenarios. Using constraints models are able to generate more context near output or generate translation, which must include the given lexically constraints within its output translation\\
Post and Vilar\cite{post} extends the Grid Beam Search model and introduces the Dynamic Beam Allocation, which improves the efficiency for lexically constrained decoding with beam search. Further Vaswani et al. \cite{vaswani} present their simple neural network architecture, called Tramsformer, based solely on attention mechanism. The transformer also achieves remarkable performance in NMT in terms of scoring, but also time efficiency.\\
These models show already very good performance in lexically constrained translation. However, non of the current research in this area is concerning about adaptive word endings in the process of sequence-to-sequence translation. As translation of the corresponding base word is done often correctly, we examine a lack of translating the correct word ending.
\newpage

\subsection*{The goals of this master's thesis}
We argue that using state of the art technologies in NMT and extend existing models (e.g. \cite{libovick}) with additional processes focusing on word endings can improve the quality of translated text. Focusing on ending of single tokens, we hope to detect patterns for generalizing endings of words better with respect to their context. Thus the high level goal of this thesis is to extend existing Decoder with a lemmatizing-approach and investigate their benefit in terms of performance improvement. The research questions that guiding this thesis are:

\begin{itemize}
	\item \textit{RQ1: To what extend is it possible to improve existing state of the art NMT by adding lemmatizer for token translation in a sequence-to-sequence model?}
	\item \textit{RQ2: Is it possible to design an approach, which can use these improvements in practise?}
\end{itemize}  

%
\subsection*{Task description}
%

The main tasks of this thesis are:

\begin{enumerate}
	\item Read up on the current state of the art in industry and research in the areas relevant to the thesis, including Attention-based models, Natural machine translator and Lexically Constrained Decoding.
	\item Get familiar with currently existing open-source solutions in the area of Multi-Source Transformer Decoding.
	\item Setup up an environment using one of these solution and integrate an score testing approach. 
	\item Develop and implement the model extension in code.
	\item Perform empirical test in order to identify research results.
	\item Writing an academic report explaining and summarizing the results from the work on items 1 to 5.	
\end{enumerate}


\subsection*{Deliverables}
%
% which are the main phases,
% 1. reading literature, existing tools


%TODO build up milestones
The major milestones of the project are as follows:

\begin{tabular}{lp{10cm}}
When & What \\
\hline\noalign{\smallskip}
$1^{\mathit{st}}$ and $2^{\mathit{st}}$ week & State of the Art review is finished. Open Source tools are examined in-depth, so the student understands them. \\
$3^{\mathit{st}}$ and $4^{\mathit{st}}$ week & Basis tool for development is extended with the testing environment and ready for implementation of the model. The model is planned more in detail.\\
$2^{\mathit{st}}$ month & First approach of proof-of-concept is implemented. \\
$3^{\mathit{nd}}$ month & Evaluation and adjustment of first version model is done. \\
$4^{\mathit{rd}}$ month & Second approach with experiments is implemented. \\
$5^{\mathit{th}}$month & The proof-of-concept implementation is finished.\\
$6^{\mathit{th}}$month & Thesis is written, proof-of-concept is fully functional, documented and delivered.
\end{tabular}

\newpage

\subsection*{Provided resources}

There is no need of special provided resources to realise this thesis

\subsection*{General thesis guidelines}

The typical rules of academic work must be followed. In
\cite{Bernstein} Bernstein describes a number of guidelines which
must be followed. At the end of the thesis, a final report has to be
written. The report should clearly be organized, follow the usual academic
report structure, and has to be written in English. As implementing software is also part of this thesis, state-of-the-art
design, coding, and documentation standards for the software have to be obeyed.

\subsection*{Advisors:}

\noindent\textbf{Professor}: \\
\noindent Prof. Dr. Martin Volk \\
\\
\noindent\textbf{Responsible assistant}: \\
\noindent Samuel L\"aubli \\

\vspace{2em}
\noindent\textbf{Signatures:}

\vspace{3\baselineskip}
\noindent Andreas Schaufelb\"uhl\hfill Prof. Dr. Martin Volk
\clearpage
\bibliographystyle{abbrv}
\bibliography{refs}

\end{document}
